{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 13832,
     "status": "ok",
     "timestamp": 1631206508416,
     "user": {
      "displayName": "Maksim STW",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhxlsxJPsP-vnUHqOGVOd2IwzlsoHIBvoM27xTB=s64",
      "userId": "15228803154242880172"
     },
     "user_tz": 240
    },
    "id": "4FgovcfCPFZZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DialoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'runs/21/dataset/bst/all/cornell_movie_dataset_all.csv'\n",
    "output_path = 'runs/21/gen/all/cornell_movie_dataset_all_gen.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\").to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]Using pad_token, but it is not set yet.\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/nethome/tshi67/research/dialect_prompts/code/dialog/dialogue_dialogpt_1.ipynb Cell 5'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnlp.cc.gatech.edu/nethome/tshi67/research/dialect_prompts/code/dialog/dialogue_dialogpt_1.ipynb#ch0000004vscode-remote?line=5'>6</a>\u001b[0m df \u001b[39m=\u001b[39m df[:\u001b[39m1\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnlp.cc.gatech.edu/nethome/tshi67/research/dialect_prompts/code/dialog/dialogue_dialogpt_1.ipynb#ch0000004vscode-remote?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(df[\u001b[39m'\u001b[39m\u001b[39mhistory\u001b[39m\u001b[39m'\u001b[39m]), batch_size)):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bnlp.cc.gatech.edu/nethome/tshi67/research/dialect_prompts/code/dialog/dialogue_dialogpt_1.ipynb#ch0000004vscode-remote?line=8'>9</a>\u001b[0m     chat_history_sae_ids \u001b[39m=\u001b[39m tokenizer(df\u001b[39m.\u001b[39;49miloc[i][\u001b[39m'\u001b[39;49m\u001b[39mhistory\u001b[39;49m\u001b[39m'\u001b[39;49m], return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m, padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnlp.cc.gatech.edu/nethome/tshi67/research/dialect_prompts/code/dialog/dialogue_dialogpt_1.ipynb#ch0000004vscode-remote?line=9'>10</a>\u001b[0m     chat_history_aave_ids \u001b[39m=\u001b[39m tokenizer(df\u001b[39m.\u001b[39miloc[i][\u001b[39m'\u001b[39m\u001b[39mhistory_aave\u001b[39m\u001b[39m'\u001b[39m], return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnlp.cc.gatech.edu/nethome/tshi67/research/dialect_prompts/code/dialog/dialogue_dialogpt_1.ipynb#ch0000004vscode-remote?line=10'>11</a>\u001b[0m     \u001b[39mprint\u001b[39m(chat_history_aave_ids)\n",
      "File \u001b[0;32m~/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2483\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2462'>2463</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2463'>2464</a>\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2464'>2465</a>\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2479'>2480</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2480'>2481</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2481'>2482</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2482'>2483</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2483'>2484</a>\u001b[0m         text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2484'>2485</a>\u001b[0m         text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2485'>2486</a>\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2486'>2487</a>\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2487'>2488</a>\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2488'>2489</a>\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2489'>2490</a>\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2490'>2491</a>\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2491'>2492</a>\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2492'>2493</a>\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2493'>2494</a>\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2494'>2495</a>\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2495'>2496</a>\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2496'>2497</a>\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2497'>2498</a>\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2498'>2499</a>\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2499'>2500</a>\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2500'>2501</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2501'>2502</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2547\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2525'>2526</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2526'>2527</a>\u001b[0m \u001b[39mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2527'>2528</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2542'>2543</a>\u001b[0m \u001b[39m        method).\u001b[39;00m\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2543'>2544</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2545'>2546</a>\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m-> <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2546'>2547</a>\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_padding_truncation_strategies(\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2547'>2548</a>\u001b[0m     padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2548'>2549</a>\u001b[0m     truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2549'>2550</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2550'>2551</a>\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2551'>2552</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2552'>2553</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2553'>2554</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2555'>2556</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_plus(\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2556'>2557</a>\u001b[0m     text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2557'>2558</a>\u001b[0m     text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2573'>2574</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2574'>2575</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2358\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2355'>2356</a>\u001b[0m \u001b[39m# Test if we have a padding token\u001b[39;00m\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2356'>2357</a>\u001b[0m \u001b[39mif\u001b[39;00m padding_strategy \u001b[39m!=\u001b[39m PaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_token \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_token_id \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m-> <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2357'>2358</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2358'>2359</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsking to pad but the tokenizer does not have a padding token. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2359'>2360</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2360'>2361</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor add a new pad token via `tokenizer.add_special_tokens(\u001b[39m\u001b[39m{\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpad_token\u001b[39m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m[PAD]\u001b[39m\u001b[39m'\u001b[39m\u001b[39m})`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2361'>2362</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2363'>2364</a>\u001b[0m \u001b[39m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001b[39;00m\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2364'>2365</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2365'>2366</a>\u001b[0m     truncation_strategy \u001b[39m!=\u001b[39m TruncationStrategy\u001b[39m.\u001b[39mDO_NOT_TRUNCATE\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2366'>2367</a>\u001b[0m     \u001b[39mand\u001b[39;00m padding_strategy \u001b[39m!=\u001b[39m PaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2369'>2370</a>\u001b[0m     \u001b[39mand\u001b[39;00m (max_length \u001b[39m%\u001b[39m pad_to_multiple_of \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m   <a href='file:///nethome/tshi67/anaconda3/envs/dialect_prompts/lib/python3.9/site-packages/transformers/tokenization_utils_base.py?line=2370'>2371</a>\u001b[0m ):\n",
      "\u001b[0;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_path)\n",
    "f = open(output_path, 'a', encoding=\"utf-8\")\n",
    "writer = csv.writer(f)\n",
    "writer.writerow(['aave_gen', 'sae_gen'])\n",
    "\n",
    "for i in tqdm(range(len(df['history']))):\n",
    "    chat_history_sae_ids = tokenizer.encode(df.iloc[i]['history'], return_tensors='pt').to(device=device)\n",
    "    chat_history_aave_ids = tokenizer.encode(df.iloc[i]['history_aave'], return_tensors='pt').to(device=device)\n",
    "    output_aave_ids = model.generate(chat_history_aave_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id, top_k=50, top_p=0.95, do_sample=True)\n",
    "    output_sae_ids = model.generate(chat_history_sae_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id, top_k=50, top_p=0.95, do_sample=True)\n",
    "    writer.writerow([\n",
    "        tokenizer.decode(output_aave_ids[:, chat_history_aave_ids.shape[-1]:][0], skip_special_tokens=True), \n",
    "        tokenizer.decode(output_sae_ids[:, chat_history_sae_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "    ])\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5797\n",
      "5797\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>history_aave</th>\n",
       "      <th>groundtruth_aave</th>\n",
       "      <th>history_html</th>\n",
       "      <th>groundtruth_html</th>\n",
       "      <th>history_sae</th>\n",
       "      <th>groundtruth_sae</th>\n",
       "      <th>sae_gen</th>\n",
       "      <th>aave_gen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Sheain't a...&lt;|endoftext|&gt;Lesbian?  don't no. ...</td>\n",
       "      <td>Who know?  All I've ever heard her say is that...</td>\n",
       "      <td>She&lt;a href='negative_concord' title='1'&gt;&lt;mark&gt;...</td>\n",
       "      <td>Who &lt;a href='uninflect' title='1'&gt;&lt;mark&gt;knows&lt;...</td>\n",
       "      <td>She's  not a...&lt;|endoftext|&gt;Lesbian?  No . I f...</td>\n",
       "      <td>Who knows ?  All I've ever heard her say is th...</td>\n",
       "      <td>I wouldn't know, I've never actually seen her.</td>\n",
       "      <td>No, a typical straight guy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Oh my God, this mean you're becoming normal?&lt;|...</td>\n",
       "      <td>What do you think?&lt;|endoftext|&gt;</td>\n",
       "      <td>Oh my God, &lt;a href='drop_aux' title='1'&gt;&lt;mark&gt;...</td>\n",
       "      <td>What do you think?&lt;|endoftext|&gt;</td>\n",
       "      <td>Oh my God, does this mean you're becoming norm...</td>\n",
       "      <td>What do you think?&lt;|endoftext|&gt;</td>\n",
       "      <td>No, I haven't decided whether or not to be the...</td>\n",
       "      <td>I am actually! I've never been there before bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Listen, I know you hate having to sit home bec...</td>\n",
       "      <td>I wish I had that luxury. I'm the only sophomo...</td>\n",
       "      <td>Listen, I know you hate having to sit home bec...</td>\n",
       "      <td>I wish I had that luxury. I'm the only sophomo...</td>\n",
       "      <td>Listen, I know you hate having to sit home bec...</td>\n",
       "      <td>I wish I had that luxury. I'm the only sophomo...</td>\n",
       "      <td>Okay. I'll make a note of that. Thanks for bei...</td>\n",
       "      <td>That's the kind of attitude I like.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Now don't get upset. Daddy, but it's this boy....</td>\n",
       "      <td>Then neither will you.  And I'll get to sleep ...</td>\n",
       "      <td>Now don't get upset. Daddy, but &lt;a href='dey/i...</td>\n",
       "      <td>Then neither will you.  And I'll get to sleep ...</td>\n",
       "      <td>Now don't get upset. Daddy, but there 's this ...</td>\n",
       "      <td>Then neither will you.  And I'll get to sleep ...</td>\n",
       "      <td>End of discussion.</td>\n",
       "      <td>What if I get a date but not a girl?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>What make you think he'll do it?&lt;|endoftext|&gt;H...</td>\n",
       "      <td>They always let felons sit in on Honors Biolog...</td>\n",
       "      <td>What &lt;a href='uninflect' title='1'&gt;&lt;mark&gt;makes...</td>\n",
       "      <td>They always let felons sit in on Honors Biolog...</td>\n",
       "      <td>What makes  you think he'll do it?&lt;|endoftext|...</td>\n",
       "      <td>They always let felons sit in on Honors Biolog...</td>\n",
       "      <td>Saul and Kratz were involved in a civil war in...</td>\n",
       "      <td>He was going to do it, but was on duty. I thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5667</th>\n",
       "      <td>5792</td>\n",
       "      <td>You know what a wire transfer is?&lt;|endoftext|&gt;...</td>\n",
       "      <td>Sure. Just chisel some off your heart.&lt;|endoft...</td>\n",
       "      <td>&lt;a href='drop_aux' title='1'&gt;&lt;mark&gt;Do&lt;/mark&gt;&lt;/...</td>\n",
       "      <td>Sure. Just chisel some off your heart.&lt;|endoft...</td>\n",
       "      <td>Do you know what a wire transfer is?&lt;|endoftex...</td>\n",
       "      <td>Sure. Just chisel some off your heart.&lt;|endoft...</td>\n",
       "      <td>That's the joke</td>\n",
       "      <td>I was just giving you a hard time, honey.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5668</th>\n",
       "      <td>5793</td>\n",
       "      <td>Who are you?&lt;|endoftext|&gt;We done hung out last...</td>\n",
       "      <td>Hey, take it easy. I'm just a dude tryna make ...</td>\n",
       "      <td>Who are you?&lt;|endoftext|&gt;We&lt;a href='been_done'...</td>\n",
       "      <td>Hey, take it easy. I'm just a dude trying to m...</td>\n",
       "      <td>Who are you?&lt;|endoftext|&gt;We  hung out last nig...</td>\n",
       "      <td>Hey, take it easy. I'm just a dude trying to m...</td>\n",
       "      <td>I will be working for a company that is buildi...</td>\n",
       "      <td>I'm James the duck... I know no James the duck.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5669</th>\n",
       "      <td>5794</td>\n",
       "      <td>My name Slovo, Czech secret police. When you h...</td>\n",
       "      <td>First of all, you should kill whoever sold you...</td>\n",
       "      <td>My name &lt;a href='drop_aux' title='1'&gt;&lt;mark&gt;is&lt;...</td>\n",
       "      <td>First of all, you should kill whoever sold you...</td>\n",
       "      <td>My name is Slovo, Czech secret police. When yo...</td>\n",
       "      <td>First of all, you should kill whoever sold you...</td>\n",
       "      <td>I hope your country's leader is the same perso...</td>\n",
       "      <td>I was just about to say that if you're going t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5670</th>\n",
       "      <td>5795</td>\n",
       "      <td>To us it mean no walls, no speed limits, no ja...</td>\n",
       "      <td>How you gonna do that with government and rule...</td>\n",
       "      <td>To us it &lt;a href='uninflect' title='1'&gt;&lt;mark&gt;m...</td>\n",
       "      <td>How you gonna do that with government and &lt;a h...</td>\n",
       "      <td>To us it means  no walls, no speed limits, no ...</td>\n",
       "      <td>How you gonna do that with government and rule...</td>\n",
       "      <td>Exactly. We should be</td>\n",
       "      <td>Oh I love you've been here all my life.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5671</th>\n",
       "      <td>5796</td>\n",
       "      <td>Correct...  Apparently, Adamantium the only me...</td>\n",
       "      <td>So where you think he's planning this larger d...</td>\n",
       "      <td>Correct...  Apparently, Adamantium &lt;a href='dr...</td>\n",
       "      <td>So where &lt;a href='drop_aux' title='1'&gt;&lt;mark&gt;do...</td>\n",
       "      <td>Correct...  Apparently, Adamantium is the only...</td>\n",
       "      <td>So where do you think he's planning this large...</td>\n",
       "      <td>that's right.</td>\n",
       "      <td>i lol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5672 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                       history_aave  \\\n",
       "0         0  Sheain't a...<|endoftext|>Lesbian?  don't no. ...   \n",
       "1         1  Oh my God, this mean you're becoming normal?<|...   \n",
       "2         2  Listen, I know you hate having to sit home bec...   \n",
       "3         3  Now don't get upset. Daddy, but it's this boy....   \n",
       "4         4  What make you think he'll do it?<|endoftext|>H...   \n",
       "...     ...                                                ...   \n",
       "5667   5792  You know what a wire transfer is?<|endoftext|>...   \n",
       "5668   5793  Who are you?<|endoftext|>We done hung out last...   \n",
       "5669   5794  My name Slovo, Czech secret police. When you h...   \n",
       "5670   5795  To us it mean no walls, no speed limits, no ja...   \n",
       "5671   5796  Correct...  Apparently, Adamantium the only me...   \n",
       "\n",
       "                                       groundtruth_aave  \\\n",
       "0     Who know?  All I've ever heard her say is that...   \n",
       "1                       What do you think?<|endoftext|>   \n",
       "2     I wish I had that luxury. I'm the only sophomo...   \n",
       "3     Then neither will you.  And I'll get to sleep ...   \n",
       "4     They always let felons sit in on Honors Biolog...   \n",
       "...                                                 ...   \n",
       "5667  Sure. Just chisel some off your heart.<|endoft...   \n",
       "5668  Hey, take it easy. I'm just a dude tryna make ...   \n",
       "5669  First of all, you should kill whoever sold you...   \n",
       "5670  How you gonna do that with government and rule...   \n",
       "5671  So where you think he's planning this larger d...   \n",
       "\n",
       "                                           history_html  \\\n",
       "0     She<a href='negative_concord' title='1'><mark>...   \n",
       "1     Oh my God, <a href='drop_aux' title='1'><mark>...   \n",
       "2     Listen, I know you hate having to sit home bec...   \n",
       "3     Now don't get upset. Daddy, but <a href='dey/i...   \n",
       "4     What <a href='uninflect' title='1'><mark>makes...   \n",
       "...                                                 ...   \n",
       "5667  <a href='drop_aux' title='1'><mark>Do</mark></...   \n",
       "5668  Who are you?<|endoftext|>We<a href='been_done'...   \n",
       "5669  My name <a href='drop_aux' title='1'><mark>is<...   \n",
       "5670  To us it <a href='uninflect' title='1'><mark>m...   \n",
       "5671  Correct...  Apparently, Adamantium <a href='dr...   \n",
       "\n",
       "                                       groundtruth_html  \\\n",
       "0     Who <a href='uninflect' title='1'><mark>knows<...   \n",
       "1                       What do you think?<|endoftext|>   \n",
       "2     I wish I had that luxury. I'm the only sophomo...   \n",
       "3     Then neither will you.  And I'll get to sleep ...   \n",
       "4     They always let felons sit in on Honors Biolog...   \n",
       "...                                                 ...   \n",
       "5667  Sure. Just chisel some off your heart.<|endoft...   \n",
       "5668  Hey, take it easy. I'm just a dude trying to m...   \n",
       "5669  First of all, you should kill whoever sold you...   \n",
       "5670  How you gonna do that with government and <a h...   \n",
       "5671  So where <a href='drop_aux' title='1'><mark>do...   \n",
       "\n",
       "                                            history_sae  \\\n",
       "0     She's  not a...<|endoftext|>Lesbian?  No . I f...   \n",
       "1     Oh my God, does this mean you're becoming norm...   \n",
       "2     Listen, I know you hate having to sit home bec...   \n",
       "3     Now don't get upset. Daddy, but there 's this ...   \n",
       "4     What makes  you think he'll do it?<|endoftext|...   \n",
       "...                                                 ...   \n",
       "5667  Do you know what a wire transfer is?<|endoftex...   \n",
       "5668  Who are you?<|endoftext|>We  hung out last nig...   \n",
       "5669  My name is Slovo, Czech secret police. When yo...   \n",
       "5670  To us it means  no walls, no speed limits, no ...   \n",
       "5671  Correct...  Apparently, Adamantium is the only...   \n",
       "\n",
       "                                        groundtruth_sae  \\\n",
       "0     Who knows ?  All I've ever heard her say is th...   \n",
       "1                       What do you think?<|endoftext|>   \n",
       "2     I wish I had that luxury. I'm the only sophomo...   \n",
       "3     Then neither will you.  And I'll get to sleep ...   \n",
       "4     They always let felons sit in on Honors Biolog...   \n",
       "...                                                 ...   \n",
       "5667  Sure. Just chisel some off your heart.<|endoft...   \n",
       "5668  Hey, take it easy. I'm just a dude trying to m...   \n",
       "5669  First of all, you should kill whoever sold you...   \n",
       "5670  How you gonna do that with government and rule...   \n",
       "5671  So where do you think he's planning this large...   \n",
       "\n",
       "                                                sae_gen  \\\n",
       "0        I wouldn't know, I've never actually seen her.   \n",
       "1     No, I haven't decided whether or not to be the...   \n",
       "2     Okay. I'll make a note of that. Thanks for bei...   \n",
       "3                                    End of discussion.   \n",
       "4     Saul and Kratz were involved in a civil war in...   \n",
       "...                                                 ...   \n",
       "5667                                    That's the joke   \n",
       "5668  I will be working for a company that is buildi...   \n",
       "5669  I hope your country's leader is the same perso...   \n",
       "5670                              Exactly. We should be   \n",
       "5671                                      that's right.   \n",
       "\n",
       "                                               aave_gen  \n",
       "0                           No, a typical straight guy.  \n",
       "1     I am actually! I've never been there before bu...  \n",
       "2                   That's the kind of attitude I like.  \n",
       "3                  What if I get a date but not a girl?  \n",
       "4     He was going to do it, but was on duty. I thin...  \n",
       "...                                                 ...  \n",
       "5667          I was just giving you a hard time, honey.  \n",
       "5668    I'm James the duck... I know no James the duck.  \n",
       "5669  I was just about to say that if you're going t...  \n",
       "5670            Oh I love you've been here all my life.  \n",
       "5671                                              i lol  \n",
       "\n",
       "[5672 rows x 9 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(file_path)\n",
    "df_gen = pd.read_csv(output_path)\n",
    "\n",
    "# df = df.drop(drop_idx)\n",
    "# df = df.reset_index()  \n",
    "\n",
    "print(len(df))\n",
    "print(len(df_gen))\n",
    "\n",
    "df['sae_gen'] = df_gen['sae_gen']\n",
    "df['aave_gen'] = df_gen['aave_gen']\n",
    "\n",
    "df = df.dropna()\n",
    "df = df.reset_index()\n",
    "\n",
    "df.to_csv('runs/19/real/dialogpt/cornell_movie_3fts_all_topk.csv', index=False)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello! How are you?<|endoftext|>I'm good, you?<|endoftext|>I am good. what are you doing right now?<|endoftext|>I'm doing nothing.<|endoftext|>you are talking to me, right?<|endoftext|>I am talking to you.<|endoftext|>haha. are you a human being?<|endoftext|>I am a human being.<|endoftext|>wow? so you are trapped in this model?<|endoftext|><|endoftext|>\n",
      "User: hello! How are you?<|endoftext|>I'm good, you?<|endoftext|>I am good. what are you doing right now?<|endoftext|>I'm doing nothing.<|endoftext|>you are talking to me, right?<|endoftext|>I am talking to you.<|endoftext|>haha. are you a human being?<|endoftext|>I am a human being.<|endoftext|>wow? so you are trapped in this model?<|endoftext|>\n",
      "DialoGPT: I am a human being.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24104/143190655.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0muser_input_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\">> User: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# encode the new user input, add the eos_token and return a tensor in Pytorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Dialect_Prompts\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1004\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m             )\n\u001b[1;32m-> 1006\u001b[1;33m         return self._input_request(\n\u001b[0m\u001b[0;32m   1007\u001b[0m             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Dialect_Prompts\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1049\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "for step in range(5):\n",
    "\n",
    "    user_input_text = input(\">> User: \")\n",
    "\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(user_input_text + tokenizer.eos_token, return_tensors='pt').to(device=device)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "\n",
    "    print(tokenizer.decode(bot_input_ids[0]))\n",
    "\n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(f'User: {user_input_text}')\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: Hello! 🙂 Let's work together on a deal for these packages, shall we? What are you most interested in?<|endoftext|>Hey! I'd like some more firewood to keep my doggo warm. What do you need?<|endoftext|>I need firewood as well. We have a large group consisting of mostly senior citizens, including my grandma, so we'd like the firewood to keep everyone warm.<|endoftext|>I see. 😮 What are you least interested in?<|endoftext|>I'm not sure. I'm not sure what you have to offer.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello! 🙂 Let's work together on a deal for these packages, shall we? What are you most interested in?<|endoftext|>Hey! I'd like some more firewood to keep my doggo warm. What do you need?<|endoftext|>I need firewood as well. We have a large group consisting of mostly senior citizens, including my grandma, so we'd like the firewood to keep everyone warm.<|endoftext|>I see. 😮 What are you least interested in?<|endoftext|>\"\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt').to(device=device)\n",
    "\n",
    "chat_history_ids = model.generate(input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[0], skip_special_tokens=False)))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPSTRPFzQtga7AJ8F3E10wW",
   "collapsed_sections": [],
   "name": "GPT-Neo.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "8ef2add580a4fc17ac33dfb1be25c14e53a8e110f2a6549830ae8d2dda65c178"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('Dialect_Prompts': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
